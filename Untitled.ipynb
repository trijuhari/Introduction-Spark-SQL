{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Library "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from pyspark.sql  import SparkSession\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T09:11:50.864712Z",
     "start_time": "2021-07-29T09:11:48.311266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "spark = SparkSession.builder().master(\"local[*]\")\n",
    "spark = SparkSession.builder() .master(\"local[*]\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T12:59:25.340513Z",
     "start_time": "2021-07-29T12:59:25.318479Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import File"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "# Create temporary view  similar with tables \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE TEMPORARY VIEW people\n",
    "USING csv \n",
    "OPTIONS (\n",
    "  path 'people.csv',\n",
    "  header true\n",
    ")\n",
    "\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T09:45:17.650320Z",
     "start_time": "2021-07-29T09:45:17.319238Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "for drop temporary exist \n",
    "spark.catalog.dropTempView(\"people\")\n",
    "\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "source": [
    "def query (query):\n",
    "    return spark.sql(f\"\"\"\n",
    "    {query}\"\"\").show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:45:08.655564Z",
     "start_time": "2021-07-29T14:45:08.642366Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Describe the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "source": [
    "query(\"DESCRIBE people \")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+-------+\n",
      "|  col_name|data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|        id|   string|   null|\n",
      "| firstName|   string|   null|\n",
      "|middleName|   string|   null|\n",
      "|  lastName|   string|   null|\n",
      "|    gender|   string|   null|\n",
      "| birthDate|   string|   null|\n",
      "|       ssn|   string|   null|\n",
      "|    salary|   string|   null|\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:45:10.499495Z",
     "start_time": "2021-07-29T14:45:10.443945Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run Simple Query "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "query(\"\"\"\n",
    "SELECT firstName, middleName, lastName, birthdate\n",
    "FROM people\n",
    "WHERE year(birthdate) > 1960 AND gender = 'F'\n",
    "ORDER BY 1\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+----------+-----------+--------------------+\n",
      "|firstName|middleName|   lastName|           birthdate|\n",
      "+---------+----------+-----------+--------------------+\n",
      "|  Adriana|   Tonisha|  Sallenger|1981-09-20T04:00:...|\n",
      "|  Adriana|     Mabel|    Dhillon|1985-02-28T05:00:...|\n",
      "|  Adriane|     Karla|  Tuminelli|1983-04-13T05:00:...|\n",
      "|  Adriene|    Mariam|  Collimore|1966-07-23T04:00:...|\n",
      "|   Ailene|    Leonor|      Yates|1984-02-08T05:00:...|\n",
      "|   Albina|  Sanjuana|    Sigmund|1974-11-20T05:00:...|\n",
      "|   Albina|     Irina| Scaplehorn|1989-04-19T04:00:...|\n",
      "|   Aletha|    Chieko|   Ledekker|1966-02-11T05:00:...|\n",
      "|   Aletha|     Justa|Pennrington|1963-03-30T05:00:...|\n",
      "|   Aletha|    Slyvia|     Surmon|1991-06-16T04:00:...|\n",
      "|     Alia|      Opal|     Havard|1994-01-21T05:00:...|\n",
      "|     Alia|     Kasha|     Coller|1971-08-08T04:00:...|\n",
      "|    Alice|    Ellena|    Entreis|1974-08-01T04:00:...|\n",
      "|    Alpha|    Zandra|   Blamphin|1969-02-21T05:00:...|\n",
      "|   Althea|     Casie|   Goncaves|1999-06-15T04:00:...|\n",
      "|   Alvina|     Leann|   Franklen|1961-11-20T05:00:...|\n",
      "|   Alyson|    Yadira|     Baudet|1991-04-05T05:00:...|\n",
      "|     Amal|     Evita|    Barbosa|1978-01-11T05:00:...|\n",
      "|     Amee|   Kirstin|     Tattam|1967-02-02T05:00:...|\n",
      "|    Ammie|   Darleen|     Messer|1974-05-23T04:00:...|\n",
      "+---------+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T12:49:55.868409Z",
     "start_time": "2021-07-29T12:49:55.467703Z"
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# More complex query\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "query(\"\"\"\n",
    "SELECT year(birthDate) as birthYear,  firstName, count (*) AS total\n",
    "FROM people\n",
    "WHERE (firstName = 'Aletha' OR firstName = 'Laila') AND gender = 'F'  \n",
    "  AND year(birthDate) > 1960\n",
    "GROUP BY birthYear, firstName\n",
    "ORDER BY birthYear, firstName\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+---------+-----+\n",
      "|birthYear|firstName|total|\n",
      "+---------+---------+-----+\n",
      "|     1963|   Aletha|    1|\n",
      "|     1964|    Laila|    1|\n",
      "|     1965|    Laila|    1|\n",
      "|     1966|   Aletha|    1|\n",
      "|     1973|    Laila|    1|\n",
      "|     1991|   Aletha|    1|\n",
      "+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T09:59:47.639620Z",
     "start_time": "2021-07-29T09:59:43.516130Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Temporary View"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T13:35:55.507149Z",
     "start_time": "2021-07-29T13:35:55.501788Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "query (\n",
    "\n",
    "\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY  VIEW WomenBornAfter1990 AS (\n",
    " SELECT firstName, middleName, lastName, year(birthDate) AS birthYear, salary \n",
    "  FROM people\n",
    "  WHERE year(birthDate) > 1990 AND gender = 'F'\n",
    ")\n",
    "\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T13:37:33.246873Z",
     "start_time": "2021-07-29T13:37:33.073449Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Temporay View"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT * FROM WomenBornAfter1990\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ").show(20,False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+----------+-----------+---------+------+\n",
      "|firstName|middleName|lastName   |birthYear|salary|\n",
      "+---------+----------+-----------+---------+------+\n",
      "|Hester   |Sherie    |Bonome     |1999     |81098 |\n",
      "|Berniece |Shelley   |Cutler     |1999     |74981 |\n",
      "|Vanessa  |Phuong    |Degoy      |1992     |94407 |\n",
      "|Suellen  |Monique   |Sorrel     |1995     |67360 |\n",
      "|Jolynn   |Yasmine   |Brolechan  |1996     |71750 |\n",
      "|Andrea   |Crystle   |Perot      |1996     |60753 |\n",
      "|Princess |Lanell    |Clibbery   |1991     |65339 |\n",
      "|Nancey   |Letitia   |Bartolijn  |1991     |86795 |\n",
      "|Ruthie   |Georgette |Magister   |1991     |70942 |\n",
      "|Cleopatra|Krystyna  |Duny       |1998     |94797 |\n",
      "|Cathrine |Shaniqua  |Burchmore  |1999     |64114 |\n",
      "|Earlene  |Johnsie   |Varren     |1993     |60104 |\n",
      "|Lisha    |Ilse      |Pealing    |1993     |56160 |\n",
      "|Tiffiny  |Danyel    |Nusche     |1997     |63782 |\n",
      "|Nicolette|Dodie     |Alyutin    |1997     |85867 |\n",
      "|Wanda    |Lorene    |Thirtle    |1998     |100707|\n",
      "|Tomasa   |Jovan     |Deny       |1998     |59807 |\n",
      "|Leida    |Kiersten  |Filippone  |1992     |89399 |\n",
      "|Bertha   |Mica      |Nolda      |1999     |52214 |\n",
      "|Jeannette|Roberta   |Tomasicchio|1994     |69778 |\n",
      "+---------+----------+-----------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:54:37.396191Z",
     "start_time": "2021-07-29T14:54:37.254324Z"
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Average"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "spark.sql(\"\"\"\n",
    "     \n",
    "SELECT round(avg(salary)) AS averageSalary\n",
    "FROM people\n",
    "\"\"\").show(100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+\n",
      "|averageSalary|\n",
      "+-------------+\n",
      "|      73410.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:56:16.407614Z",
     "start_time": "2021-07-29T14:56:16.239188Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Export ssa_names.csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# Create temporary view  similar with tables \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE OR REPLACE  TEMPORARY VIEW ssa_names\n",
    "USING csv \n",
    "OPTIONS (\n",
    "  path 'ssa_names.csv',\n",
    "  header true\n",
    ")\n",
    "\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 122
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:43:43.888249Z",
     "start_time": "2021-07-29T14:43:43.621839Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    "SELECT * FROM  ssa_names\n",
    "\"\"\"\n",
    ").show(20)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+------+-----+----+\n",
      "|firstName|gender|total|year|\n",
      "+---------+------+-----+----+\n",
      "|Jennifer |F     |54336|1983|\n",
      "|Jessica  |F     |45278|1983|\n",
      "|Amanda   |F     |33752|1983|\n",
      "|Ashley   |F     |33292|1983|\n",
      "|Sarah    |F     |27228|1983|\n",
      "|Melissa  |F     |23472|1983|\n",
      "|Nicole   |F     |22392|1983|\n",
      "|Stephanie|F     |22323|1983|\n",
      "|Heather  |F     |20749|1983|\n",
      "|Elizabeth|F     |19838|1983|\n",
      "|Crystal  |F     |17904|1983|\n",
      "|Amy      |F     |17095|1983|\n",
      "|Michelle |F     |16828|1983|\n",
      "|Tiffany  |F     |15960|1983|\n",
      "|Kimberly |F     |15374|1983|\n",
      "|Christina|F     |15359|1983|\n",
      "|Amber    |F     |14886|1983|\n",
      "|Erin     |F     |14835|1983|\n",
      "|Rebecca  |F     |14711|1983|\n",
      "|Rachel   |F     |14588|1983|\n",
      "+---------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:43:56.945459Z",
     "start_time": "2021-07-29T14:43:56.798073Z"
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create  SSADistinctNames Temporary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "query(\n",
    "\"\"\"\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW SSADistinctNames AS \n",
    "  SELECT DISTINCT firstName AS ssaFirstName \n",
    "  FROM ssa_names;\n",
    "\"\"\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:35:38.853085Z",
     "start_time": "2021-07-29T14:35:38.809197Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create  PeopleDistinctNames Temporary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "query(\n",
    "\"\"\"\n",
    "\n",
    "CREATE OR REPLACE TEMPORARY VIEW PeopleDistinctNames AS \n",
    "  SELECT DISTINCT firstName \n",
    "  FROM people;\n",
    "  \n",
    "  \n",
    "  \n",
    "\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T14:36:15.654847Z",
     "start_time": "2021-07-29T14:36:15.599956Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Join 2 Temporay View"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "spark.sql(\n",
    "\"\"\"\n",
    " \n",
    "  SELECT firstName \n",
    "FROM PeopleDistinctNames \n",
    "INNER JOIN SSADistinctNames ON firstName = ssaFirstName\n",
    "\"\"\").show(20,False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n",
      "|firstName|\n",
      "+---------+\n",
      "|Trina    |\n",
      "|Shannon  |\n",
      "|Shawn    |\n",
      "|Carolyn  |\n",
      "|Sue      |\n",
      "|Grace    |\n",
      "|Tiara    |\n",
      "|Susana   |\n",
      "|Cassondra|\n",
      "|Lynsey   |\n",
      "|Alison   |\n",
      "|Robyn    |\n",
      "|Andrea   |\n",
      "|Shalonda |\n",
      "|Adriana  |\n",
      "|Tori     |\n",
      "|Nadia    |\n",
      "|Jacquelyn|\n",
      "|Yadira   |\n",
      "|Kristina |\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T15:02:16.973549Z",
     "start_time": "2021-07-29T15:02:15.048694Z"
    },
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT count(firstName) \n",
    "FROM PeopleDistinctNames\n",
    "WHERE firstName IN (\n",
    "  SELECT ssaFirstName FROM SSADistinctNames\n",
    ")\n",
    "\"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------+\n",
      "|count(firstName)|\n",
      "+----------------+\n",
      "|             209|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T15:04:17.942864Z",
     "start_time": "2021-07-29T15:04:14.612020Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import json file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "# Create temporary view  similar with tables \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "CREATE OR REPLACE  TEMPORARY VIEW databricks_blog\n",
    "USING json \n",
    "OPTIONS (\n",
    "  path 'databricks-blog.json',\n",
    "  header true\n",
    ")\n",
    "\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "metadata": {},
     "execution_count": 163
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T15:07:43.286083Z",
     "start_time": "2021-07-29T15:07:42.522828Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Describe  json file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "spark.sql(\"\"\"\n",
    "DESCRIBE databricks_blog\n",
    "\"\"\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------+--------------------+-------+\n",
      "|   col_name|           data_type|comment|\n",
      "+-----------+--------------------+-------+\n",
      "|    authors|       array<string>|   null|\n",
      "| categories|       array<string>|   null|\n",
      "|    content|              string|   null|\n",
      "|    creator|              string|   null|\n",
      "|      dates|struct<createdOn:...|   null|\n",
      "|description|              string|   null|\n",
      "|         id|              bigint|   null|\n",
      "|       link|              string|   null|\n",
      "|       slug|              string|   null|\n",
      "|     status|              string|   null|\n",
      "|      title|              string|   null|\n",
      "+-----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T15:14:40.803291Z",
     "start_time": "2021-07-29T15:14:40.727690Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dot Notation in Spark SQL"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT title, \n",
    "       CAST(dates.publishedOn AS timestamp) AS publishedOn \n",
    "FROM databricks_blog\n",
    "\"\"\").show( truncate=False )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------------------------+-------------------+\n",
      "|title                                                                               |publishedOn        |\n",
      "+------------------------------------------------------------------------------------+-------------------+\n",
      "|MapR Integrates the Complete Apache Spark Stack                                     |2014-04-10 00:00:00|\n",
      "|Apache Spark 0.9.1 Released                                                         |2014-04-10 00:00:00|\n",
      "|Application Spotlight: Alpine Data Labs                                             |2014-04-01 00:00:00|\n",
      "|Spark SQL: Manipulating Structured Data Using Apache Spark                          |2014-03-27 00:00:00|\n",
      "|Apache Spark 0.9.0 Released                                                         |2014-02-04 00:00:00|\n",
      "|Apache Spark In MapReduce (SIMR)                                                    |2014-01-02 00:00:00|\n",
      "|Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time           |2014-03-26 00:00:00|\n",
      "|Apache Spark: A Delight for Developers                                              |2014-03-21 00:00:00|\n",
      "|Databricks announces \"Certified on Apache Spark\" Program                            |2014-03-19 00:00:00|\n",
      "|Apache Spark Now a Top-level Apache Project                                         |2014-03-03 00:00:00|\n",
      "|AMPLab updates the Big Data Benchmark                                               |2014-02-13 00:00:00|\n",
      "|Databricks at the O'Reilly Strata Conference 2014                                   |2014-02-11 00:00:00|\n",
      "|Apache Spark and Hadoop: Working Together                                           |2014-01-22 00:00:00|\n",
      "|Apache Spark 0.8.1 Released                                                         |2013-12-20 00:00:00|\n",
      "|Highlights From Spark Summit 2013                                                   |2013-12-19 00:00:00|\n",
      "|Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications|2013-11-22 00:00:00|\n",
      "|Databricks and Cloudera Partner to Support Apache Spark                             |2013-10-29 00:00:00|\n",
      "|The Growing Apache Spark Community                                                  |2013-10-28 00:00:00|\n",
      "|Databricks and the Apache Spark Platform                                            |2013-10-27 00:00:00|\n",
      "|Databricks and MapR                                                                 |2014-04-11 00:00:00|\n",
      "+------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T16:53:26.468545Z",
     "start_time": "2021-07-29T16:53:26.303447Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter Function with Horizontal View"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT authors , categories,\n",
    "TRANSFORM (categories, category  -> LOWER(category)) AS lwr_categories\n",
    "FROM  databricks_blog\n",
    "\"\"\").show(5, truncate=False, vertical=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-RECORD 0------------------------------------------------------------\n",
      " authors        | [Tomer Shiran (VP of Product Management at MapR)]  \n",
      " categories     | [Company Blog, Partners]                           \n",
      " lwr_categories | [company blog, partners]                           \n",
      "-RECORD 1------------------------------------------------------------\n",
      " authors        | [Tathagata Das]                                    \n",
      " categories     | [Apache Spark, Engineering Blog, Machine Learning] \n",
      " lwr_categories | [apache spark, engineering blog, machine learning] \n",
      "-RECORD 2------------------------------------------------------------\n",
      " authors        | [Steven Hillion]                                   \n",
      " categories     | [Company Blog, Partners]                           \n",
      " lwr_categories | [company blog, partners]                           \n",
      "-RECORD 3------------------------------------------------------------\n",
      " authors        | [Michael Armbrust, Reynold Xin]                    \n",
      " categories     | [Apache Spark, Engineering Blog]                   \n",
      " lwr_categories | [apache spark, engineering blog]                   \n",
      "-RECORD 4------------------------------------------------------------\n",
      " authors        | [Patrick Wendell]                                  \n",
      " categories     | [Apache Spark, Engineering Blog]                   \n",
      " lwr_categories | [apache spark, engineering blog]                   \n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T16:49:58.140111Z",
     "start_time": "2021-07-29T16:49:57.955389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter Function with Vertical View"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT title,\n",
    "  FILTER (categories, category -> category = \"Apache Spark\") filtered\n",
    "FROM databricks_blog\n",
    "wher\n",
    "\"\"\").show( truncate=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------------------------+--------------+\n",
      "|title                                                                               |filtered      |\n",
      "+------------------------------------------------------------------------------------+--------------+\n",
      "|MapR Integrates the Complete Apache Spark Stack                                     |[]            |\n",
      "|Apache Spark 0.9.1 Released                                                         |[Apache Spark]|\n",
      "|Application Spotlight: Alpine Data Labs                                             |[]            |\n",
      "|Spark SQL: Manipulating Structured Data Using Apache Spark                          |[Apache Spark]|\n",
      "|Apache Spark 0.9.0 Released                                                         |[Apache Spark]|\n",
      "|Apache Spark In MapReduce (SIMR)                                                    |[Apache Spark]|\n",
      "|Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time           |[]            |\n",
      "|Apache Spark: A Delight for Developers                                              |[Apache Spark]|\n",
      "|Databricks announces \"Certified on Apache Spark\" Program                            |[]            |\n",
      "|Apache Spark Now a Top-level Apache Project                                         |[Apache Spark]|\n",
      "|AMPLab updates the Big Data Benchmark                                               |[Apache Spark]|\n",
      "|Databricks at the O'Reilly Strata Conference 2014                                   |[]            |\n",
      "|Apache Spark and Hadoop: Working Together                                           |[Apache Spark]|\n",
      "|Apache Spark 0.8.1 Released                                                         |[Apache Spark]|\n",
      "|Highlights From Spark Summit 2013                                                   |[]            |\n",
      "|Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications|[Apache Spark]|\n",
      "|Databricks and Cloudera Partner to Support Apache Spark                             |[]            |\n",
      "|The Growing Apache Spark Community                                                  |[]            |\n",
      "|Databricks and the Apache Spark Platform                                            |[]            |\n",
      "|Databricks and MapR                                                                 |[]            |\n",
      "+------------------------------------------------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T16:54:17.062219Z",
     "start_time": "2021-07-29T16:54:16.943501Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT title,\n",
    "  FILTER (categories, category -> category = \"Apache Spark\") filtered\n",
    "FROM databricks_blog\n",
    "\n",
    "\"\"\").show( truncate=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------------------------+--------------+\n",
      "|title                                                                               |filtered      |\n",
      "+------------------------------------------------------------------------------------+--------------+\n",
      "|MapR Integrates the Complete Apache Spark Stack                                     |[]            |\n",
      "|Apache Spark 0.9.1 Released                                                         |[Apache Spark]|\n",
      "|Application Spotlight: Alpine Data Labs                                             |[]            |\n",
      "|Spark SQL: Manipulating Structured Data Using Apache Spark                          |[Apache Spark]|\n",
      "|Apache Spark 0.9.0 Released                                                         |[Apache Spark]|\n",
      "|Apache Spark In MapReduce (SIMR)                                                    |[Apache Spark]|\n",
      "|Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time           |[]            |\n",
      "|Apache Spark: A Delight for Developers                                              |[Apache Spark]|\n",
      "|Databricks announces \"Certified on Apache Spark\" Program                            |[]            |\n",
      "|Apache Spark Now a Top-level Apache Project                                         |[Apache Spark]|\n",
      "|AMPLab updates the Big Data Benchmark                                               |[Apache Spark]|\n",
      "|Databricks at the O'Reilly Strata Conference 2014                                   |[]            |\n",
      "|Apache Spark and Hadoop: Working Together                                           |[Apache Spark]|\n",
      "|Apache Spark 0.8.1 Released                                                         |[Apache Spark]|\n",
      "|Highlights From Spark Summit 2013                                                   |[]            |\n",
      "|Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications|[Apache Spark]|\n",
      "|Databricks and Cloudera Partner to Support Apache Spark                             |[]            |\n",
      "|The Growing Apache Spark Community                                                  |[]            |\n",
      "|Databricks and the Apache Spark Platform                                            |[]            |\n",
      "|Databricks and MapR                                                                 |[]            |\n",
      "+------------------------------------------------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T17:00:36.411511Z",
     "start_time": "2021-07-29T17:00:36.316548Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exist Function with Vertical View"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT title,\n",
    "  EXISTS (authors, author -> author = \"Reynold Xin\" OR  author = \"Ion Stoica\") selected\n",
    "FROM databricks_blog\n",
    "\"\"\").show( truncate=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------------------------+--------+\n",
      "|title                                                                               |selected|\n",
      "+------------------------------------------------------------------------------------+--------+\n",
      "|MapR Integrates the Complete Apache Spark Stack                                     |false   |\n",
      "|Apache Spark 0.9.1 Released                                                         |false   |\n",
      "|Application Spotlight: Alpine Data Labs                                             |false   |\n",
      "|Spark SQL: Manipulating Structured Data Using Apache Spark                          |true    |\n",
      "|Apache Spark 0.9.0 Released                                                         |false   |\n",
      "|Apache Spark In MapReduce (SIMR)                                                    |false   |\n",
      "|Sharethrough Uses Apache Spark Streaming to Optimize Bidding in Real Time           |false   |\n",
      "|Apache Spark: A Delight for Developers                                              |false   |\n",
      "|Databricks announces \"Certified on Apache Spark\" Program                            |false   |\n",
      "|Apache Spark Now a Top-level Apache Project                                         |true    |\n",
      "|AMPLab updates the Big Data Benchmark                                               |true    |\n",
      "|Databricks at the O'Reilly Strata Conference 2014                                   |false   |\n",
      "|Apache Spark and Hadoop: Working Together                                           |true    |\n",
      "|Apache Spark 0.8.1 Released                                                         |false   |\n",
      "|Highlights From Spark Summit 2013                                                   |false   |\n",
      "|Putting Apache Spark to Use: Fast In-Memory Computing for Your Big Data Applications|false   |\n",
      "|Databricks and Cloudera Partner to Support Apache Spark                             |true    |\n",
      "|The Growing Apache Spark Community                                                  |false   |\n",
      "|Databricks and the Apache Spark Platform                                            |true    |\n",
      "|Databricks and MapR                                                                 |false   |\n",
      "+------------------------------------------------------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-29T17:02:29.321120Z",
     "start_time": "2021-07-29T17:02:28.786539Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}